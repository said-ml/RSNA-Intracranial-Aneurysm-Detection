# ğŸ”‘ Keys to Reliable Cross-Validation (RSNA & Beyond)

A strong cross-validation (CV) setup is the **backbone of leaderboard success** in RSNA and any Kaggle competition.  
Below are the 10 golden rules for building a **trustworthy and LB-correlated CV**.

---

## 1. Stratification + Grouping
- âœ… Use **`StratifiedGroupKFold`**:
  - **Stratify** on `Aneurysm Present` â†’ balances positives/negatives per fold.
  - **Group** by `patient_id` â†’ prevents leakage (same patient in both train & val).
- âš ï¸ Without grouping â†’ CV will leak and look artificially inflated.

---

## 2. Multiple Seeds
- One split with `random_state=42` is **not enough**.
- Run CV with **3â€“5 different seeds** and average results.
- This reduces variance caused by â€œluckyâ€ or â€œunluckyâ€ splits.
- âœ… Final score = **mean OOF across seeds**.

---

## 3. OOF Predictions (Out-of-Fold)
- Always save **OOF predictions** (`shape = [N, num_labels]`).
- Benefits:
  - Recompute RSNA AUC **offline exactly**.
  - Essential for **ensembling/stacking** later.
- âœ… Strong OOF â†” Public LB correlation = reliable CV.

---

## 4. Consistent Preprocessing in CV & Submission
- Do **not preprocess test differently** from train.
- If you use `.npz` conversion â†’ apply same logic in CV.
- âš ï¸ Avoid mismatches (e.g. train with `64` slices, predict with `32` slices).

---

## 5. Patient-Level Holdout Test
- After CV, hold out **5â€“10% patients** (never touched in CV).
- Train on the rest, evaluate once on holdout.
- âœ… This provides a **true generalization estimate** before submitting to Kaggle.

---

## 6. Monitor Per-Class AUC
- RSNA metric: `"Aneurysm Present"` weighted **13Ã— more**.
- But track **all 14 AUCs** in validation.
- Improving small classes often stabilizes LB correlation.

---

## 7. Blend k-Folds Instead of Choosing Best Fold
- âŒ Never pick â€œFold 3 looked bestâ€ â†’ thatâ€™s **cherry-picking**.
- âœ… Train separate models for each fold.
- âœ… Average (or weighted average) predictions across folds.
- This reduces fold noise and matches Kaggle ensemble behavior.

---

## 8. Reproduce LB Distribution
- LB = evaluated on **hidden patients**.
- Replicate the **same patient scan distribution** in CV:
  - Example: Some patients have 20 scans, others only 3.
  - Keep ratios balanced across folds.

---

## 9. Error Analysis
- After CV:
  - Inspect **worst patients/scans**.
  - Identify **systematic failure modes** (e.g. consistent errors in Basilar Tip).
- âœ… Fixing systematic issues often yields a **bigger LB jump** than tuning layers.

---

## 10. Check Correlation with LB
- After first submissions:
  - Compare **OOF vs Public LB**.
  - Track correlation coefficient (**Ï**).
- âœ… High correlation (`Ï > 0.9`) â†’ CV is reliable.
- âš ï¸ Low correlation (`Ï < 0.7`) â†’ redesign CV split strategy.

---

## ğŸ“Œ Summary
Reliable CV =  
**Stratified + Grouped Splits** â **Multiple Seeds** â **OOF Predictions** â **Consistent Preprocessing** â **Holdout Test** â **Error Analysis**.  

Master this cycle â†’ your offline validation will **mirror Kaggle LB** and youâ€™ll iterate with confidence.
