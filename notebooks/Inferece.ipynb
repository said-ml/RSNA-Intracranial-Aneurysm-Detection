{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4bc386e5-a464-41be-84df-d9be1f93226a",
   "metadata": {},
   "source": [
    "## Inference Pepline:\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "9b1af2c9-f8d7-4b77-82b6-8ab855b32b6c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-04T05:10:10.482398Z",
     "start_time": "2025-11-04T05:10:10.401206Z"
    }
   },
   "source": [
    "\n",
    "## 1-tools:\n",
    "# tools imports\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import polars as pl\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "import sys\n",
    "\n",
    "try:\n",
    "    import kaggle_evaluation.rsna_inference_server\n",
    "except:\n",
    "    sys.path.append('/kaggle/input/rsna-intracranial-aneurysm-detection')\n",
    "    import kaggle_evaluation.rsna_inference_server\n",
    "\n",
    "import gc  # garbage collection\n",
    "from typing import List, Tuple, Optional, Dict  # hints\n",
    "\n",
    "print('imports ok')"
   ],
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'kaggle_evaluation'",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mModuleNotFoundError\u001B[39m                       Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[2]\u001B[39m\u001B[32m, line 19\u001B[39m\n\u001B[32m     18\u001B[39m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[32m---> \u001B[39m\u001B[32m19\u001B[39m     \u001B[38;5;28;01mimport\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01mkaggle_evaluation\u001B[39;00m\u001B[34;01m.\u001B[39;00m\u001B[34;01mrsna_inference_server\u001B[39;00m\n\u001B[32m     20\u001B[39m \u001B[38;5;28;01mexcept\u001B[39;00m:\n",
      "\u001B[31mModuleNotFoundError\u001B[39m: No module named 'kaggle_evaluation'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001B[31mModuleNotFoundError\u001B[39m                       Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[2]\u001B[39m\u001B[32m, line 22\u001B[39m\n\u001B[32m     20\u001B[39m \u001B[38;5;28;01mexcept\u001B[39;00m:\n\u001B[32m     21\u001B[39m     sys.path.append(\u001B[33m'\u001B[39m\u001B[33m/kaggle/input/rsna-intracranial-aneurysm-detection\u001B[39m\u001B[33m'\u001B[39m)\n\u001B[32m---> \u001B[39m\u001B[32m22\u001B[39m     \u001B[38;5;28;01mimport\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01mkaggle_evaluation\u001B[39;00m\u001B[34;01m.\u001B[39;00m\u001B[34;01mrsna_inference_server\u001B[39;00m\n\u001B[32m     24\u001B[39m \u001B[38;5;28;01mimport\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01mgc\u001B[39;00m  \u001B[38;5;66;03m# garbage collection\u001B[39;00m\n\u001B[32m     25\u001B[39m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01mtyping\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m List, Tuple, Optional, Dict  \u001B[38;5;66;03m# hints\u001B[39;00m\n",
      "\u001B[31mModuleNotFoundError\u001B[39m: No module named 'kaggle_evaluation'"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "cell_type": "markdown",
   "id": "530ad58b-0a85-4f26-b95e-f0840ab86030",
   "metadata": {},
   "source": "## 1 Pydicom .dcm files to .nzp files¶"
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c3810584-b3bd-4c2f-a6bf-6faea47cf3b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda is availabe that is True\n"
     ]
    }
   ],
   "source": [
    "import os, glob\n",
    "import numpy as np\n",
    "import pydicom\n",
    "from scipy.ndimage import zoom\n",
    "\n",
    "# -------------------------\n",
    "# Config\n",
    "# -------------------------\n",
    "TARGET_SHAPE = (32, 256, 256)  # (D,H,W)\n",
    "WINDOWS = [(40, 80), (600, 2800)]  # WL, WW\n",
    "BBOX_MARGIN = 10\n",
    "\n",
    "# -------------------------\n",
    "# Helpers\n",
    "# -------------------------\n",
    "def dcm_key_sorter(path):\n",
    "    \"\"\"Sort slices by InstanceNumber or Z position.\"\"\"\n",
    "    try:\n",
    "        ds = pydicom.dcmread(path, stop_before_pixels=True)\n",
    "        if \"InstanceNumber\" in ds:\n",
    "            return int(ds.InstanceNumber)\n",
    "        if \"ImagePositionPatient\" in ds:\n",
    "            return float(ds.ImagePositionPatient[2])\n",
    "    except:\n",
    "        pass\n",
    "    return path\n",
    "\n",
    "def load_dicom_volume(series_dir):\n",
    "    \"\"\"Load all slices of a DICOM series as a 3D numpy array.\"\"\"\n",
    "    files = sorted(glob.glob(os.path.join(series_dir, \"*.dcm\")), key=dcm_key_sorter)\n",
    "    slices = []\n",
    "    for fp in files:\n",
    "        try:\n",
    "            ds = pydicom.dcmread(fp, force=True)\n",
    "            arr = ds.pixel_array.astype(np.float32)\n",
    "            if hasattr(ds, \"RescaleSlope\") and hasattr(ds, \"RescaleIntercept\"):\n",
    "                arr = arr * float(ds.RescaleSlope) + float(ds.RescaleIntercept)\n",
    "            if arr.ndim == 3:\n",
    "                arr = arr[..., 0]\n",
    "            slices.append(arr)\n",
    "        except:\n",
    "            continue\n",
    "    if not slices:\n",
    "        raise ValueError(f\"No valid DICOMs found in {series_dir}\")\n",
    "    return np.stack(slices, axis=0).astype(np.float32)  # (D,H,W)\n",
    "\n",
    "def apply_window(vol, WL, WW):\n",
    "    \"\"\"Apply intensity window.\"\"\"\n",
    "    lower, upper = WL - WW/2.0, WL + WW/2.0\n",
    "    clipped = np.clip(vol, lower, upper)\n",
    "    return ((clipped - lower) / max(1e-12, (upper - lower))).astype(np.float32)\n",
    "\n",
    "def center_crop(vol, target_shape):\n",
    "    \"\"\"Crop or pad to match target shape at center.\"\"\"\n",
    "    d,h,w = vol.shape\n",
    "    td,th,tw = target_shape\n",
    "    out = np.zeros(target_shape, dtype=vol.dtype)\n",
    "\n",
    "    d0 = max((d-td)//2,0); d1 = d0+td\n",
    "    h0 = max((h-th)//2,0); h1 = h0+th\n",
    "    w0 = max((w-tw)//2,0); w1 = w0+tw\n",
    "\n",
    "    ds = max((td-d)//2,0); de = ds+d\n",
    "    hs = max((th-h)//2,0); he = hs+h\n",
    "    ws = max((tw-w)//2,0); we = ws+w\n",
    "\n",
    "    out[ds:de, hs:he, ws:we] = vol[d0:d1, h0:h1, w0:w1]\n",
    "    return out\n",
    "\n",
    "def resize_volume(vol, target_shape):\n",
    "    \"\"\"Resize to target shape using scipy zoom.\"\"\"\n",
    "    factors = [t/c for t,c in zip(target_shape, vol.shape)]\n",
    "    return zoom(vol, factors, order=1).astype(np.float32)\n",
    "\n",
    "# -------------------------\n",
    "# Main processor\n",
    "# -------------------------\n",
    "def process_series_to_npz(series_dir, target_shape=TARGET_SHAPE, windows=WINDOWS):\n",
    "    \"\"\"\n",
    "    Load dicom series -> apply windowing -> resize -> return npz-like dict.\n",
    "    Returns dict with 'image' key for inference.\n",
    "    \"\"\"\n",
    "    vol = load_dicom_volume(series_dir)  # (D,H,W)\n",
    "\n",
    "    # Windowed channels\n",
    "    chans = [apply_window(vol, wl, ww) for wl, ww in windows]  # list of (D,H,W)\n",
    "\n",
    "    # Stack channels\n",
    "    vol_cdhw = np.stack(chans, axis=0)  # (C,D,H,W)\n",
    "\n",
    "    # Resize each channel to target\n",
    "    C = vol_cdhw.shape[0]\n",
    "    out = np.zeros((C, *target_shape), dtype=np.float32)\n",
    "    for c in range(C):\n",
    "        out[c] = resize_volume(vol_cdhw[c], target_shape)\n",
    "    #print(f'/////////////////////volume shape ={out.shape}////////////////////////')\n",
    "    out = out[:1] #<============ this line is added\n",
    "    return  out#{\"image\": out}"
   ]
  },
  {
   "cell_type": "code",
   "id": "63a085bd-86d6-4831-8174-90d63a67e22c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-04T05:16:30.892194Z",
     "start_time": "2025-11-04T05:16:30.890437Z"
    }
   },
   "source": "##  2-ReUse The training setting:",
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "##  2-ReUse The training setting:",
   "id": "e88a2f74cb9dd37e"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-04T05:17:10.122671Z",
     "start_time": "2025-11-04T05:17:10.121260Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import pydicom, glob, os\n",
    "from scipy.ndimage import zoom\n",
    "\n",
    "# ----------------------------\n",
    "# Reuse your training settings\n",
    "# ----------------------------\n",
    "TARGET_SHAPE = (32, 256, 256)\n",
    "WINDOWS = [(40, 80), (600, 2800)]\n",
    "DEVICE = \"cuda\"\n",
    "\n",
    "def apply_window(vol, WL, WW):\n",
    "    lower = WL - WW/2.0\n",
    "    upper = WL + WW/2.0\n",
    "    clipped = np.clip(vol, lower, upper)\n",
    "    scaled = (clipped - lower) / max(1e-12, (upper - lower))\n",
    "    return scaled.astype(np.float32)\n",
    "\n",
    "def resample_volume(vol, target_shape):\n",
    "    dz = target_shape[0] / vol.shape[0]\n",
    "    dh = target_shape[1] / vol.shape[1]\n",
    "    dw = target_shape[2] / vol.shape[2]\n",
    "    return zoom(vol, (dz, dh, dw), order=1)\n",
    "\n",
    "def load_dicom_series(series_path):\n",
    "    files = sorted(glob.glob(os.path.join(series_path, \"*.dcm\")))\n",
    "    slices = []\n",
    "    for f in files:\n",
    "        ds = pydicom.dcmread(f)\n",
    "        arr = ds.pixel_array.astype(np.float32)\n",
    "        if hasattr(ds, \"RescaleSlope\") and hasattr(ds, \"RescaleIntercept\"):\n",
    "            arr = arr * float(ds.RescaleSlope) + float(ds.RescaleIntercept)\n",
    "        slices.append(arr)\n",
    "    vol = np.stack(slices, axis=0)\n",
    "    return vol\n",
    "\n",
    "def preprocess_series(series_path):\n",
    "    vol = load_dicom_series(series_path)\n",
    "\n",
    "    # multi-window channels\n",
    "    chans = []\n",
    "    for wl, ww in WINDOWS:\n",
    "        chans.append(apply_window(vol, wl, ww))\n",
    "    vol_cdhw = np.stack(chans, axis=0)  # (C,D,H,W)\n",
    "\n",
    "    # resample each channel\n",
    "    out = np.zeros((vol_cdhw.shape[0], *TARGET_SHAPE), dtype=np.float32)\n",
    "    for c in range(vol_cdhw.shape[0]):\n",
    "        out[c] = resample_volume(vol_cdhw[c], TARGET_SHAPE)\n",
    "\n",
    "    # scale back to [0,1]\n",
    "    out = np.clip(out, 0, 1)\n",
    "\n",
    "    return out\n",
    "\n",
    "def run_inference(model, series_path, class_names):\n",
    "    arr = preprocess_series(series_path)\n",
    "    x = torch.tensor(arr, dtype=torch.float32).unsqueeze(0).to(DEVICE)  # (1,C,D,H,W)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        logits = model(x)\n",
    "        probs = torch.sigmoid(logits).cpu().numpy()[0]\n",
    "\n",
    "    return dict(zip(class_names, probs))\n",
    "\n"
   ],
   "id": "ba73d058834566a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "8655eb703c53f59c"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 3-Model Definition:\n",
   "id": "d8fc09b20d3b294f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# this models is borrow from https://github.com/Tencent/MedicalNet/blob/master/models/resnet.py\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from functools import partial\n",
    "\n",
    "__all__ = [\n",
    "    'resnet10', 'resnet18', 'resnet34', 'resnet50',\n",
    "    'resnet101', 'resnet152', 'resnet200'\n",
    "]\n",
    "\n",
    "def conv3x3x3(in_planes, out_planes, stride=1, dilation=1):\n",
    "    \"\"\"3x3x3 convolution with padding\"\"\"\n",
    "    return nn.Conv3d(\n",
    "        in_planes,\n",
    "        out_planes,\n",
    "        kernel_size=3,\n",
    "        dilation=dilation,\n",
    "        stride=stride,\n",
    "        padding=dilation,\n",
    "        bias=False\n",
    "    )\n",
    "\n",
    "\n",
    "def downsample_basic_block(x, planes, stride):\n",
    "    \"\"\"Downsample using avg pooling + zero padding\"\"\"\n",
    "    out = F.avg_pool3d(x, kernel_size=1, stride=stride)\n",
    "    zero_pads = torch.zeros(\n",
    "        out.size(0), planes - out.size(1), out.size(2), out.size(3), out.size(4),\n",
    "        device=x.device, dtype=x.dtype\n",
    "    )\n",
    "    out = torch.cat([out, zero_pads], dim=1)\n",
    "    return out\n",
    "\n",
    "\n",
    "class BasicBlock(nn.Module):\n",
    "    expansion = 1\n",
    "    def __init__(self, inplanes, planes, stride=1, dilation=1, downsample=None):\n",
    "        super().__init__()\n",
    "        self.conv1 = conv3x3x3(inplanes, planes, stride, dilation)\n",
    "        self.bn1 = nn.BatchNorm3d(planes)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.conv2 = conv3x3x3(planes, planes, dilation=dilation)\n",
    "        self.bn2 = nn.BatchNorm3d(planes)\n",
    "        self.downsample = downsample\n",
    "\n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "        out = self.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.bn2(self.conv2(out))\n",
    "        if self.downsample is not None:\n",
    "            residual = self.downsample(x)\n",
    "        out = self.relu(out + residual)\n",
    "        return out\n",
    "\n",
    "\n",
    "class Bottleneck(nn.Module):\n",
    "    expansion = 4\n",
    "    def __init__(self, inplanes, planes, stride=1, dilation=1, downsample=None):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv3d(inplanes, planes, kernel_size=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm3d(planes)\n",
    "\n",
    "        self.conv2 = nn.Conv3d(\n",
    "            planes, planes, kernel_size=3, stride=stride, padding=dilation,\n",
    "            dilation=dilation, bias=False\n",
    "        )\n",
    "        self.bn2 = nn.BatchNorm3d(planes)\n",
    "\n",
    "        self.conv3 = nn.Conv3d(planes, planes * 4, kernel_size=1, bias=False)\n",
    "        self.bn3 = nn.BatchNorm3d(planes * 4)\n",
    "\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.downsample = downsample\n",
    "\n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "        out = self.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.relu(self.bn2(self.conv2(out)))\n",
    "        out = self.bn3(self.conv3(out))\n",
    "        if self.downsample is not None:\n",
    "            residual = self.downsample(x)\n",
    "        out = self.relu(out + residual)\n",
    "        return out\n",
    "\n",
    "\n",
    "class ResNet3D(nn.Module):\n",
    "    def __init__(self, block, layers, num_classes=1, in_channels=1, segmentation= not  False):\n",
    "        super().__init__()\n",
    "        self.inplanes = 64\n",
    "        self.segmentation = segmentation\n",
    "\n",
    "        self.conv1 = nn.Conv3d(in_channels, 64, kernel_size=7,\n",
    "                               stride=(2, 2, 2), padding=(3, 3, 3), bias=False)\n",
    "        self.bn1 = nn.BatchNorm3d(64)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.maxpool = nn.MaxPool3d(kernel_size=3, stride=2, padding=1)\n",
    "\n",
    "        self.layer1 = self._make_layer(block, 64, layers[0])\n",
    "        self.layer2 = self._make_layer(block, 128, layers[1], stride=2)\n",
    "        self.layer3 = self._make_layer(block, 256, layers[2], stride=2)\n",
    "        self.layer4 = self._make_layer(block, 512, layers[3], stride=2)\n",
    "\n",
    "        self.avgpool = nn.AdaptiveAvgPool3d((1, 1, 1))\n",
    "        self.fc = nn.Linear(512 * block.expansion, num_classes)\n",
    "\n",
    "        if self.segmentation:\n",
    "            #self.seg_head = nn.Conv3d(512 * block.expansion, num_classes, kernel_size=1)\n",
    "            self.seg_head = nn.Sequential(  # this for resnet18\n",
    "                nn.ConvTranspose3d(512, 256, 2, stride=2),\n",
    "                nn.ReLU(inplace=True),\n",
    "                nn.ConvTranspose3d(256, 128, 2, stride=2),\n",
    "                nn.ReLU(inplace=True),\n",
    "                nn.ConvTranspose3d(128, 64, 2, stride=2),\n",
    "                nn.ReLU(inplace=True),\n",
    "                nn.Conv3d(64, num_classes, 1)\n",
    "            )\n",
    "            self.seg_head2 = nn.Sequential(  # this for resnet50\n",
    "                nn.ConvTranspose3d(\n",
    "                    2048, 32, kernel_size=2, stride=2\n",
    "                ),\n",
    "                nn.BatchNorm3d(32),\n",
    "                nn.ReLU(inplace=True),\n",
    "                nn.Conv3d(32, 32, kernel_size=3, padding=1, bias=False),\n",
    "                nn.BatchNorm3d(32),\n",
    "                nn.ReLU(inplace=True),\n",
    "                nn.Conv3d(32, 14, kernel_size=1, bias=False)\n",
    "            )\n",
    "\n",
    "    def _make_layer(self, block, planes, blocks, stride=1, dilation=1):\n",
    "        downsample = None\n",
    "        if stride != 1 or self.inplanes != planes * block.expansion:\n",
    "            downsample = nn.Sequential(\n",
    "                nn.Conv3d(self.inplanes, planes * block.expansion,\n",
    "                          kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm3d(planes * block.expansion)\n",
    "            )\n",
    "\n",
    "        layers = []\n",
    "        layers.append(block(self.inplanes, planes, stride, dilation, downsample))\n",
    "        self.inplanes = planes * block.expansion\n",
    "        for _ in range(1, blocks):\n",
    "            layers.append(block(self.inplanes, planes, dilation=dilation))\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x=x.float()\n",
    "        x_orig = x  # save original input\n",
    "        x = self.relu(self.bn1(self.conv1(x)))\n",
    "        x = self.maxpool(x)\n",
    "\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        x = self.layer4(x)\n",
    "\n",
    "        # classification\n",
    "        cls_out = self.avgpool(x)\n",
    "        cls_out = torch.flatten(cls_out, 1)\n",
    "        cls_out = self.fc(cls_out)\n",
    "\n",
    "        # segmentation\n",
    "        seg_out = None\n",
    "        if self.segmentation:\n",
    "            seg_out = self.seg_head(x)\n",
    "            # interpolate to match ORIGINAL input size\n",
    "            seg_out = F.interpolate(\n",
    "                seg_out,\n",
    "                size=x_orig.shape[2:],  # <-- key: use original input shape\n",
    "                mode='trilinear',\n",
    "                align_corners=False\n",
    "            )\n",
    "            return cls_out.float(), seg_out.float()\n",
    "        else:\n",
    "            return cls_out.float()\n",
    "\n",
    "\n",
    "# Factory functions\n",
    "def resnet10(**kwargs):\n",
    "    return ResNet3D(BasicBlock, [1,1,1,1], **kwargs)\n",
    "\n",
    "def resnet18(**kwargs):\n",
    "    return ResNet3D(BasicBlock, [2,2,2,2], **kwargs)\n",
    "\n",
    "def resnet34(**kwargs):\n",
    "    return ResNet3D(BasicBlock, [3,4,6,3], **kwargs)\n",
    "\n",
    "def resnet50(**kwargs):\n",
    "    return ResNet3D(Bottleneck, [3,4,6,3], **kwargs)\n",
    "\n",
    "def resnet101(**kwargs):\n",
    "    return ResNet3D(Bottleneck, [3,4,23,3], **kwargs)\n",
    "\n",
    "def resnet152(**kwargs):\n",
    "    return ResNet3D(Bottleneck, [3,8,36,3], **kwargs)\n",
    "\n",
    "def resnet200(**kwargs):\n",
    "    return ResNet3D(Bottleneck, [3,24,36,3], **kwargs)"
   ],
   "id": "59d9d57fffeb2e74"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 4 Laoding Pretarnined models( .pt checkpoints):",
   "id": "acc5c02cb2ca50df"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def load_series_as_array(series_path):\n",
    "    \"\"\"Load hidden test DICOM series → (D,H,W) float32.\"\"\"\n",
    "    dcm_files = sorted(glob.glob(f\"{series_path}/*.dcm\"))\n",
    "    slices = []\n",
    "    for f in dcm_files:\n",
    "        dcm = pydicom.dcmread(f)\n",
    "        img = dcm.pixel_array.astype(np.float32)\n",
    "        slices.append(img)\n",
    "\n",
    "    if not slices:\n",
    "        return None\n",
    "\n",
    "    volume = np.stack(slices, axis=0)\n",
    "    volume = np.resize(volume, TARGET_SHAPE).astype(np.float32)\n",
    "\n",
    "    # Suppose 'x' is your 4D array: [D, H, W]\n",
    "    #x = torch.tensor(x, dtype=torch.float32)         # [D,H,W]\n",
    "    #x = x.unsqueeze(0).unsqueeze(0)                 # [1,1,D,H,W] (batch + channel)\n",
    "    #x = x.to(DEVICE)\n",
    "    return volume\n",
    "\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "def set_seed(seed=42):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)  # if using multi-GPU\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "\n",
    "set_seed(42)\n",
    "ckpt_path ='/kaggle/input/cv-all58/best_model.pt'\n",
    "device ='cuda'\n",
    "num_classes = 14\n",
    "def _load_model_for_fold(model, ckpt_path, device, num_classes):\n",
    "    model = model(num_classes=num_classes).to(device)\n",
    "    model.eval()\n",
    "    ckpt = torch.load(ckpt_path,\n",
    "                      map_location=device,\n",
    "                      weights_only=True)\n",
    "\n",
    "    state_dict = ckpt.get(\"model_state_dict\", ckpt)\n",
    "    if any(k.startswith(\"module.\") for k in state_dict.keys()):\n",
    "        state_dict = {k.replace(\"module.\", \"\", 1): v for k, v in state_dict.items()}\n",
    "\n",
    "    missing, unexpected = model.load_state_dict(state_dict, strict=False)\n",
    "    if missing or unexpected:\n",
    "        print(f\"[WARN] load_state_dict(strict=False) reported:\")\n",
    "        print(f\"Missing keys ({len(missing)}): {missing[:10]}{' ...' if len(missing) > 10 else ''}\")\n",
    "        print(f\"Unexpected keys ({len(unexpected)}): {unexpected[:10]}{' ...' if len(unexpected) > 10 else ''}\")\n",
    "\n",
    "    return model\n"
   ],
   "id": "d11952abbe27b964"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 5-Making The Prediction:",
   "id": "eedd86b2034d473d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# ==============\n",
    "# Inference API\n",
    "# ==============\n",
    "import polars as pl\n",
    "LABEL_COLS = [\n",
    "    \"Left Infraclinoid Internal Carotid Artery\",\n",
    "    \"Right Infraclinoid Internal Carotid Artery\",\n",
    "    \"Left Supraclinoid Internal Carotid Artery\",\n",
    "    \"Right Supraclinoid Internal Carotid Artery\",\n",
    "    \"Left Middle Cerebral Artery\",\n",
    "    \"Right Middle Cerebral Artery\",\n",
    "    \"Anterior Communicating Artery\",\n",
    "    \"Left Anterior Cerebral Artery\",\n",
    "    \"Right Anterior Cerebral Artery\",\n",
    "    \"Left Posterior Communicating Artery\",\n",
    "    \"Right Posterior Communicating Artery\",\n",
    "    \"Basilar Tip\",\n",
    "    \"Other Posterior Circulation\",\n",
    "    \"Aneurysm Present\",\n",
    "]\n",
    "set_seed(42)\n",
    "ckpt_path ='/kaggle/input/resnet18-75epochs/best_model.pt'\n",
    "device ='cuda'\n",
    "num_classes = 14\n",
    "USE_AMP=True\n",
    "DEVICE ='cuda'\n",
    "from torch import amp\n",
    "@torch.no_grad()\n",
    "def predict(series_path: str) -> pl.DataFrame:\n",
    "    \"\"\"Server calls this function. Assumes global `model` and `processor` are ready.\"\"\"\n",
    "\n",
    "    model = _load_model_for_fold(resnet18, ckpt_path, device, num_classes).float()\n",
    "    import torch.nn as nn\n",
    "\n",
    "    # Change conv1 from in_channels=1 → 2   <===this is added\n",
    "\n",
    "    # Replace conv1 with in_channels=2\n",
    "    '''\n",
    "    model.conv1 = nn.Conv3d(\n",
    "        in_channels=2,\n",
    "        out_channels=64,\n",
    "        kernel_size=(7, 7, 7),\n",
    "        stride=(2, 2, 2),\n",
    "        padding=(3, 3, 3),\n",
    "        bias=False\n",
    "    ).to(device).type(next(model.parameters()).dtype)  # <-- match dtype\n",
    "\n",
    "    model.conv1 = nn.Conv3d(\n",
    "    in_channels=1, out_channels=64,\n",
    "    kernel_size=(7, 7, 7), stride=(2, 2, 2), padding=(3, 3, 3), bias=False\n",
    "     ).to(device).type(next(model.parameters()).dtype)  # <-- match dtype\n",
    "    '''\n",
    "    #model = model.float()\n",
    "    #print('\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\')\n",
    "    try:\n",
    "        # CPU preprocessing\n",
    "        #print(f'process_series_to_npz={process_series_to_npz(series_path)}')\n",
    "        volume = process_series_to_npz(series_path)\n",
    "        #volume = load_series_as_array(series_path)\n",
    "          # (D,H,W) in [0,1]\n",
    "        volume_tensor = torch.from_numpy(volume).unsqueeze(0).to(DEVICE)  # (1,1,D,H,W)\n",
    "        # Suppose x is your volume: [D,H,W]\n",
    "        #volume  = torch.tensor(volume , dtype=torch.float32)  # [D,H,W]\n",
    "\n",
    "        # Add channel and batch dimensions\n",
    "        #volume = volume_tensor.unsqueeze(0)#.unsqueeze(0)           # [1,1,D,H,W]\n",
    "\n",
    "        # Move to device\n",
    "        #volume  = volume.to(DEVICE)\n",
    "\n",
    "        print(f'volume shape ={volume.shape}')\n",
    "        # Forward pass\n",
    "        with amp.autocast(device_type='cuda', enabled=USE_AMP):\n",
    "              logits = model(volume_tensor)# (1,14)\n",
    "        # added code\n",
    "        if isinstance(logits, tuple):   # e.g., (cls, seg)\n",
    "            logits = logits[0]  # take classification output\n",
    "            ##############\n",
    "\n",
    "            logits = torch.sigmoid(logits)\n",
    "            probs = torch.sigmoid(logits).cpu().numpy().flatten().tolist()\n",
    "\n",
    "        result_df = pl.DataFrame(data=[probs], schema=LABEL_COLS, orient='row')\n",
    "\n",
    "        # Cleanup\n",
    "        del volume_tensor\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.empty_cache()\n",
    "        gc.collect()\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"[Predict] Error: {e}\")\n",
    "        result_df = pl.DataFrame(data=[[0.5] * len(LABEL_COLS)], schema=LABEL_COLS, orient='row')\n",
    "\n",
    "    # Remove shared temp if present\n",
    "    import shutil\n",
    "    shutil.rmtree('/kaggle/shared', ignore_errors=True)\n",
    "    return result_df\n"
   ],
   "id": "3e4e6faf8ac8b016"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 6-Run THe Evaluation Server:\n",
   "id": "f15dcec7cc9b4600"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import sys\n",
    "try:\n",
    "          import kaggle_evaluation.rsna_inference_server\n",
    "          inference_server = kaggle_evaluation.rsna_inference_server.RSNAInferenceServer(predict)\n",
    "except:\n",
    "          sys.path.append('/kaggle/input/rsna-intracranial-aneurysm-detection/kaggle_evaluation')\n",
    "          import kaggle_evaluation.rsna_inference_server\n",
    "          inference_server = kaggle_evaluation.rsna_inference_server.RSNAInferenceServer(predict)\n",
    "\n",
    "#sys.path.append('/kaggle/input/rsna-intracranial-aneurysm-detection/kaggle_evaluation')\n",
    "\n",
    "import shutil, os\n",
    "\n",
    "#shared_dir = \"/kaggle/shared\"\n",
    "#if os.path.exists(shared_dir):\n",
    "    #shutil.rmtree(shared_dir)   # delete it completely\n",
    "#os.makedirs(shared_dir, exist_ok=True)\n",
    "if os.getenv('KAGGLE_IS_COMPETITION_RERUN'):\n",
    "    inference_server.serve()\n",
    "else:\n",
    "    sys.path.append('/kaggle/input/rsna-intracranial-aneurysm-detection/kaggle_evaluation')\n",
    "    inference_server.run_local_gateway()\n",
    "    try:\n",
    "        sub = pl.read_parquet('/kaggle/working/submission.parquet')\n",
    "        import pandas as pd\n",
    "\n",
    "        #sub = pd.read_parquet(\"submission.parquet\")\n",
    "        #print(sub.dtypes)\n",
    "        #print(sub.head())\n",
    "        #print(\"Shape:\", sub.shape)\n",
    "        #sub = pl.read_parquet('/kaggle/working/submission.parquet')\n",
    "        print(sub.head())\n",
    "    except Exception as e:\n",
    "        print(f\"Submission parquet not found yet: {e}\")"
   ],
   "id": "da18639cdbbac986"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
